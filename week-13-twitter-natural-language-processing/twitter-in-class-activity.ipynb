{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Langauge Processing & API Data Access in Python\n",
    "\n",
    "In this lesson you will learn how to take a set of tweets and clean them in order to \n",
    "analyze the frequency of words found in the tweets. You will learn how to do several things \n",
    "including:\n",
    "\n",
    "1. Remove url's from tweets\n",
    "1. Clean up tweet text including differences in case that will affect unique word counts \n",
    "2. Summarize and count individual and sets of words found in tweets\n",
    "\n",
    "You can use this notebook as the basis for your homework for this week. One option to make things easier is to simply adjust the code in this notebook, and remove the cells that you do not need for your analysis. \n",
    "\n",
    "\n",
    "## Get and Analyze Tweets - Natural Language Processing in Python\n",
    "\n",
    "When you work with social media and other text data the user community creates and\n",
    "curates the content. This means there are NO RULES! This also means that you may\n",
    "have to perform extra steps to clean the data to ensure you are analyzing the right\n",
    "thing.\n",
    "\n",
    "Next, you will explore the text associated with a set of tweets that you access using tweepy and the twitter API. You will use some standard natural language processing (also known as text mining) approaches to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "import re\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to define your keys: \n",
    "\n",
    "```python \n",
    "consumer_key= 'yourkeyhere'\n",
    "consumer_secret= 'yourkeyhere'\n",
    "access_token= 'yourkeyhere'\n",
    "access_token_secret= 'yourkeyhere'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have authenticated, you can access twitter. If you app is read + write permissions you can even post a tweet.\n",
    "\n",
    "Give it a try now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```# Post a tweet from Python\n",
    "api.update_status(\"Look, I'm tweeting from #Python in my #earthanalytics class! @EarthLabCU\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've authenticated you're ready to search for tweets that contain `#climatechange`. Below you grab 1,000 recent tweets filtering out retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"#climate+change -filter:retweets\"\n",
    "\n",
    "tweets = tw.Cursor(api.search,\n",
    "                   q=search_term,\n",
    "                   lang=\"en\",\n",
    "                   since='2018-04-23').items(1000)\n",
    "\n",
    "all_tweets = [tweet.text for tweet in tweets]\n",
    "all_tweets[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Twitter Data - Remove URL's (links) From Each Tweet\n",
    "\n",
    "The tweets above have some elements that you do not want in your word counts. For instance you don't want to include url's in your analysis. You can remove url's using regular expressions accessed from the `re` package. Regular expressions are a special syntax that is used to identify patterns in a text string. While this lesson will not cover regular expressions, below - this syntax:\n",
    "\n",
    "`([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)`\n",
    "\n",
    "tells Python to find all strings that look like a url, and replace it with nothing --  `\"\"`. It also removes other punctionation including hashtags - `#`.\n",
    "\n",
    "`re.sub` allows you to substitute a selection of characters defined using a regular expression, with something else. \n",
    "\n",
    "In the function below, this line takes the text in each tweet and replaces the url with `\"\"` (nothing).\n",
    "\n",
    "`re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", tweet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    \"\"\"Replace url's found in a \n",
    "    text string with nothing (ie it will remove the url from \n",
    "    the string.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call the function in a list comprehension. A list comprehension is a clever way to implement a `for loop` in python - when you want the output of the `for loop` to be a list. Below you loop through each tweet in the all_tweets object, and you apply the `remove_url()` function to it. This removes urls from each item in your list of tweets and creates a new list with the cleaned tweets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets_no_urls = [remove_url(tweet) for tweet in all_tweets]\n",
    "all_tweets_no_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list comprehension above, does the same thing that this line does\n",
    "# But with less code and removing the .append() call which can be slower\n",
    "my_tweets = []\n",
    "for tweet in all_tweets:\n",
    "    my_tweets.append(remove_url(tweet))\n",
    "    \n",
    "my_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleanup - Address Case Issues\n",
    "\n",
    "Capitalization is also a challenge when analyzing text data. If you are trying to create a list of unique words in your tweets, words with capitalization will be different from words that are all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how capitalization impacts unique returned values\n",
    "ex_list = [\"Dog\", \"dog\", \"dog\", \"cat\", \"cat\", \",\"]\n",
    "# Get unique elements in the list\n",
    "set(ex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for this you can make each word lowercase using the string method `.lower()`. Below that method is applied using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note how capitalization impacts unique returned values\n",
    "words_list = [\"Dog\", \"dog\", \"dog\", \"cat\", \"cat\", \",\"]\n",
    "# Make all elements in the list lowercase\n",
    "\n",
    "lower_case = [word.lower() for word in words_list]\n",
    "# Get unique elements in the list\n",
    "lower_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the words in your list are lower case. Thus the `set()` function will return only unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have only unique words\n",
    "set(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create List of Words from Tweets\n",
    "\n",
    "Right now you have a list of lists that contains each full tweet. However to do a word frequency analysis, you need a list of all of the words associated with each tweet. You can use `.split()` to split out each word into a unique element in a list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words from one tweet into unique elements\n",
    "all_tweets_no_urls[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you will notice above that you have a capital word in your list of words. You can combine `.lower()` with `.split()` to remove capital letters and split up the tweet in one step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the words from one tweet into unique elements\n",
    "all_tweets_no_urls[0].lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split words in all of the tweets, you can then string both methods together in a list comprehension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sublist of words for each tweet, all lower case\n",
    "words_in_tweet = [tweet.lower().split() for tweet in all_tweets_no_urls]\n",
    "words_in_tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords From Tweet Text With `nltk`\n",
    "\n",
    "The `python` package `nltk` is commonly used for text analysis. Included in this package is a list of \"stop words\", these include commonly appearing words such as  who, what, you, ect. that generally don't add meaningful information to the text you are trying to analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# View a few words from the set\n",
    "list(stop_words)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the stop words provided by `nltk` are all lower-case. This works well given you already have converted all of your tweet words to lower case using the `python` `string` method `.lower()`. Next you will remove all stop words from each tweet. First, have a look at the words in the first tweet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_in_tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you remove all of the stop words in each tweet. The list comprehension below might look confusing as it's nested. The list comprehension below is the same as calling:\n",
    "\n",
    "```python\n",
    "for all_words in words_in_tweet:\n",
    "    for a word in all_words:\n",
    "        # remove stop words\n",
    "```\n",
    "\n",
    "Compare the words in that tweet, to the words in the tweet once the stop words are removed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from each tweet list of words\n",
    "tweets_nsw = [[word for word in tweet_words if not word in stop_words]\n",
    "              for tweet_words in words_in_tweet]\n",
    "tweets_nsw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Collection Words\n",
    "\n",
    "In additional to removing stopwords it's common to also remove collection words. Collection words are the words that you used to query your data from twitter. This case you used `climate change` as a collection term. Thus you can expect that these terms will be found in each tweet. This could skew your word frequency analysis. \n",
    "\n",
    "Remove the words - climate, change, and climatechange - from the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_words = ['climatechange', 'climate', 'change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_nsw_nc = [[w for w in word if not w in collection_words]\n",
    "                 for word in tweets_nsw]\n",
    "tweets_nsw_nc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Word Frequency\n",
    "\n",
    "Now that you have cleaned up your data, you are ready to calculate word frequencies. To begin, flatten your list. Note that you could flatten your list with another list comprehension like this:\n",
    "\n",
    "`all_words = [item for sublist in tweets_nsw for item in sublist]`\n",
    "\n",
    "But it's actually faster to use itertools to flatten the list as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(itertools.chain(*tweets_nsw))\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_nocollect = [item for sublist in tweets_nsw_nc for item in sublist]\n",
    "len(all_words_nocollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a list of words from your tweets. Remember that from this sample the average tweet length was about 16 words, before the stop words are removed, so this number seems reasonable. \n",
    "\n",
    "To get the count of how many times each words appears in the sample you can use the built-in `Python` library `collections`, it helps create a special type of a `Python dictonary.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_with_collection_words = collections.Counter(all_words)\n",
    "type(counts_with_collection_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookat the counts for your data including the collection words. Notice that the words climate, change and climatechange are prevalent in your analysis given they were a collection term. Thus, it likely does make sense to remove them from this analysis.\n",
    "\n",
    "The `collection.Counter` object has a useful built-in method `most_common` that will return the most commonly used words, and the number of times that they are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View word counts for list that includes collection terms\n",
    "counts_with_collection_words.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View word counts for list that does NOT INCLUDE collection terms\n",
    "cleaned_tweet_word_list = collections.Counter(all_words_nocollect)\n",
    "cleaned_tweet_word_list.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out the number of unique words you can take the len of the object counts you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_tweet_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, turn your list of words into a pandas dictionary for analysis and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_words = pd.DataFrame.from_dict(cleaned_tweet_word_list,\n",
    "                                        orient='index').reset_index()\n",
    "df_tweet_words.columns = ['words', 'count']\n",
    "df_tweet_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort words highest count to lowest\n",
    "sorted_df = df_tweet_words.sort_values(by='count',\n",
    "                                       ascending=False)\n",
    "# Select the top 15 most common words\n",
    "sorted_df_s = sorted_df[:16]\n",
    "\n",
    "# Sort the data from smallest to largest value for plotting\n",
    "sorted_df_s = sorted_df_s.sort_values(by='count', ascending=True)\n",
    "sorted_df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "#ax.bar(sorted_df_s['words'], sorted_df_s['count'], color = 'purple');\n",
    "\n",
    "sorted_df_s.plot.barh(x='words',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "ax.set_title(\"Common Words Found in Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "A sentiment analysis is an approach to classifying sets of words. The most basic sentiment analysis will classify tweets to see if the message is for example generally positive or negative. This type of analysis might be useful for instance if you are trying to understand:\n",
    "\n",
    "1. Feelings about the current political environment\n",
    "2. General sentiment around say a movie or a book review, or even a restaurante\n",
    "\n",
    "and more. To perform a sentiment analysis it's good to remove urls as you did before just in case there are words that might throw off a sentiment analysis. However you don't need to worry about case or splitting each word into a new item in a list. The analysis will be performed on all words in each tweet.\n",
    "\n",
    "To begin the sentiment analysis, you can use the list of tweets with url's removed created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All tweets with URL's removed\n",
    "all_tweets_no_urls[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TextBlob()` function is used to turn individual tweets into a TextBlob object that can be analyzed in many ways. One way is to apply the `sentiment.polarity` on each TextBlob object which returns a sentiment value that ranges from positive (1) to negative (-1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_tweets = [TextBlob(tweet) for tweet in all_tweets_no_urls]\n",
    "tb_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the textblob object, run `sentiment.polarity` on each tweet. Notice that below you use a list comprehension again to create a list that contains \n",
    "\n",
    "1. the tweet sentiment value (ranging between -1 : 1 and\n",
    "2. the tweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each tweet by sentiment\n",
    "tweet_sentiment = [[tweet.sentiment.polarity, str(tweet)] for tweet in tb_tweets]\n",
    "tweet_sentiment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to create a pandas dataframe from your list of tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sentiment_df = pd.DataFrame(tweet_sentiment, columns=[\"polarity\", \"tweet\"])\n",
    "tweet_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your data\n",
    "fig, ax = plt.subplots()\n",
    "tweet_sentiment_df.hist(bins=[-1, -.5, -.2, .2, .5, 1],\n",
    "                        ax=ax,\n",
    "                        color=\"purple\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:earth-analytics-python]",
   "language": "python",
   "name": "conda-env-earth-analytics-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
